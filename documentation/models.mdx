---
title: "Available Models"
description: "Explore all models available through the Morpheus Inference API"
---

# Available Models

The Morpheus Inference Marketplace provides access to a variety of open-source AI models. Models are hosted by providers in the decentralized marketplace, and availability may vary based on provider activity.

<Info>
**Pricing:** The Morpheus Inference API is currently **FREE** during the Open Beta program. Billing infrastructure will be implemented soon, with free inference available until 1/31/26.
</Info>

## Large Language Models (LLMs)

### Flagship Models

These are the most capable models available for complex tasks.

| Model | Context Window | Capabilities | Best For |
|-------|---------------|--------------|----------|
| `qwen3-coder-480b-a35b-instruct` | 256K | Code, Function Calling | Code generation, programming |
| `qwen3-coder-480b-a35b-instruct:web` | 256K | Code, Function Calling, Web | Code with web search |
| `hermes-3-llama-3.1-405b` | 128K | Web Search | General purpose, instruction following |
| `hermes-3-llama-3.1-405b:web` | 128K | Web Search | General purpose with web |
| `gpt-oss-120b` | 128K | Function Calling | GPT-style responses |
| `gpt-oss-120b:web` | 128K | Function Calling, Web | GPT-style with web search |

### Reasoning Models

Models optimized for step-by-step thinking and complex problem solving.

| Model | Context Window | Capabilities | Best For |
|-------|---------------|--------------|----------|
| `kimi-k2-thinking` | 256K | Code, Function Calling, Reasoning | Deep reasoning, math, logic, coding |
| `kimi-k2-thinking:web` | 256K | Code, Function Calling, Reasoning, Web | Deep reasoning with web search |
| `glm-4.7-thinking` | 198K | Function Calling, Reasoning | Extended thinking, analysis |
| `glm-4.7-thinking:web` | 198K | Function Calling, Reasoning, Web | Extended thinking with web |
| `qwen3-235b` | 128K | Function Calling | Complex reasoning, long documents |
| `qwen3-235b:web` | 128K | Function Calling, Web | Complex reasoning with web |

### Mid-Size Models

Balanced performance and speed for most use cases.

| Model | Context Window | Capabilities | Best For |
|-------|---------------|--------------|----------|
| `llama-3.3-70b` | 128K | Function Calling | General purpose, reliable |
| `llama-3.3-70b:web` | 128K | Function Calling, Web | General purpose with web |
| `qwen3-next-80b` | 256K | Function Calling | Next-gen reasoning, long context |
| `qwen3-next-80b:web` | 256K | Function Calling, Web | Next-gen with web search |
| `mistral-31-24b` | 128K | Function Calling, Vision | Fast, efficient, image analysis |
| `mistral-31-24b:web` | 128K | Function Calling, Vision, Web | Fast with web search |
| `glm-4.6` | 198K | Function Calling, Reasoning | General purpose, long context |
| `glm-4.6:web` | 198K | Function Calling, Reasoning, Web | General purpose with web |
| `glm-4.7` | 198K | Function Calling, Reasoning | Improved GLM, largest context |
| `glm-4.7:web` | 198K | Function Calling, Reasoning, Web | Improved GLM with web |
| `venice-uncensored` | 32K | — | Uncensored, creative, roleplay |
| `venice-uncensored:web` | 32K | Web | Uncensored with web search |
| `hermes-4-14b` | 128K | — | Efficient instruction following |

### Fast Models

Optimized for speed and low latency.

| Model | Context Window | Capabilities | Best For |
|-------|---------------|--------------|----------|
| `llama-3.2-3b` | 128K | Function Calling | Fastest responses, simple tasks |
| `llama-3.2-3b:web` | 128K | Function Calling, Web | Fast with web search |
| `qwen3-4b` | 32K | Function Calling, Reasoning | Lightweight, mobile, low-latency |
| `qwen3-4b:web` | 32K | Function Calling, Reasoning, Web | Lightweight with web |

## Embeddings Models

For vector embeddings and semantic search.

| Model | Best For |
|-------|----------|
| `text-embedding-bge-m3` | Text embeddings, RAG, semantic search |

## Audio Models

### Text-to-Speech

| Model | Best For |
|-------|----------|
| `tts-kokoro` | Natural-sounding voice synthesis |

### Speech-to-Text

| Model | Best For |
|-------|----------|
| `whisper-v3-large-turbo` | Transcription, audio processing |

## Model Capabilities

<AccordionGroup>
<Accordion title="Function Calling">
Models with function calling can invoke tools and APIs. Use the `tools` parameter in your chat completion request to define available functions.

**Supported models:** Most models except `venice-uncensored` and `hermes-3-llama-3.1-405b`
</Accordion>

<Accordion title="Reasoning">
Reasoning models support extended thinking and step-by-step problem solving. They're optimized for complex math, logic, and analytical tasks.

**Supported models:** `kimi-k2-thinking`, `glm-4.7-thinking`, `glm-4.6`, `glm-4.7`, `qwen3-4b`
</Accordion>

<Accordion title="Vision">
Vision-capable models can analyze images passed in the messages array.

**Supported models:** `mistral-31-24b`
</Accordion>

<Accordion title="Web Search">
Models with the `:web` suffix can search the internet for current information.

**Supported models:** All models have a `:web` variant
</Accordion>

<Accordion title="Code Optimization">
Models specifically optimized for code generation and programming tasks.

**Supported models:** `qwen3-coder-480b-a35b-instruct`, `kimi-k2-thinking`
</Accordion>
</AccordionGroup>

## Model Naming Convention

Models with the `:web` suffix have web search capabilities enabled, allowing them to access current information from the internet.

| Suffix | Meaning |
|--------|---------|
| (none) | Base model without web access |
| `:web` | Model with web search capabilities |

## Using Models

Specify the model ID in your API requests:

<Tabs>
<Tab title="curl">
```bash
curl https://api.mor.org/api/v1/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-3.3-70b",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ]
  }'
```
</Tab>

<Tab title="Python">
```python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_API_KEY",
    base_url="https://api.mor.org/api/v1"
)

response = client.chat.completions.create(
    model="llama-3.3-70b",
    messages=[
        {"role": "user", "content": "Hello!"}
    ]
)
```
</Tab>

<Tab title="JavaScript">
```javascript
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "YOUR_API_KEY",
  baseURL: "https://api.mor.org/api/v1",
});

const response = await client.chat.completions.create({
  model: "llama-3.3-70b",
  messages: [
    { role: "user", content: "Hello!" }
  ],
});
```
</Tab>
</Tabs>

## List Active Models

Query the API to see currently available models:

```bash
curl https://api.mor.org/api/v1/models \
  -H "Authorization: Bearer YOUR_API_KEY"
```

<Tip>
Model availability depends on active providers in the Morpheus Inference Marketplace. The API automatically routes your request to the highest-rated provider for your selected model.
</Tip>

## Model Selection Guide

<AccordionGroup>
<Accordion title="Best for coding">
- **`qwen3-coder-480b-a35b-instruct`** - Top choice for code generation (256K context)
- **`kimi-k2-thinking`** - Best for complex algorithmic problems with reasoning
- **`llama-3.3-70b`** - Good balance of speed and quality
</Accordion>

<Accordion title="Best for long documents">
- **`qwen3-next-80b`** - 256K context window
- **`qwen3-coder-480b-a35b-instruct`** - 256K context window
- **`glm-4.7`** - 198K context, excellent at document analysis
- **`kimi-k2-thinking`** - 256K context with reasoning
</Accordion>

<Accordion title="Best for speed">
- **`qwen3-4b`** - Fastest, 32K context
- **`llama-3.2-3b`** - Very fast, 128K context
- **`mistral-31-24b`** - Good speed with vision support
</Accordion>

<Accordion title="Best for reasoning">
- **`kimi-k2-thinking`** - Deep reasoning chains, 256K context
- **`glm-4.7-thinking`** - Extended thinking mode, 198K context
- **`qwen3-235b`** - Complex analysis, 128K context
</Accordion>

<Accordion title="Best for uncensored/creative">
- **`venice-uncensored`** - Minimal content restrictions, roleplay
</Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={2}>
<Card title="Quickstart" icon="rocket" href="/quickstart">
  Get started making your first API call.
</Card>

<Card title="Chat Completions" icon="message" href="/api-reference/chat/completions">
  Full API reference for chat completions.
</Card>

<Card title="Embeddings" icon="magnifying-glass" href="/api-reference/embeddings/create-embeddings">
  Create embeddings for semantic search.
</Card>

<Card title="Text-to-Speech" icon="volume-high" href="/api-reference/audio/speech">
  Generate speech from text.
</Card>
</CardGroup>
