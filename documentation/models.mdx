---
title: "Available Models"
description: "Explore all models available through the Morpheus Inference API"
---

# Available Models

The Morpheus Inference Marketplace provides access to a variety of open-source AI models. Models are hosted by providers in the decentralized marketplace, and availability may vary based on provider activity.

<Info>
**Pricing:** The Morpheus Inference API is currently **FREE** during the Open Beta program. Billing infrastructure will be implemented soon, with free inference available until 1/31/26.
</Info>

## Large Language Models (LLMs)

### Flagship Models

These are the most capable models available for complex tasks.

| Model | Context Window | Capabilities | Best For |
|-------|---------------|--------------|----------|
| `minimax-m2.5` | 1M | Code, Function Calling, Vision, Reasoning | AI agents, autonomous workflows, multi-step tool orchestration |
| `qwen3-coder-480b-a35b-instruct` | 256K | Code, Function Calling | Code generation, programming |
| `hermes-3-llama-3.1-405b` | 128K | — | General purpose, instruction following |
| `gpt-oss-120b` | 128K | Function Calling | GPT-style responses |

### Reasoning Models

Models optimized for step-by-step thinking and complex problem solving.

| Model | Context Window | Capabilities | Best For |
|-------|---------------|--------------|----------|
| `glm-5` | 200K | Code, Function Calling, Reasoning | Agentic engineering, complex systems, long-horizon tasks |
| `kimi-k2.5` | 256K | Code, Function Calling, Reasoning, Vision | Math, visual reasoning, parallel agent workflows |
| `kimi-k2-thinking` | 256K | Code, Function Calling, Reasoning | Deep reasoning, math, logic, coding |
| `glm-4.7-thinking` | 198K | Function Calling, Reasoning | Extended thinking, analysis |
| `qwen3-235b` | 128K | Function Calling | Complex reasoning, long documents |

### Mid-Size Models

Balanced performance and speed for most use cases.

| Model | Context Window | Capabilities | Best For |
|-------|---------------|--------------|----------|
| `llama-3.3-70b` | 128K | Function Calling | General purpose, reliable |
| `qwen3-next-80b` | 256K | Function Calling | Next-gen reasoning, long context |
| `mistral-31-24b` | 128K | Function Calling, Vision | Fast, efficient, image analysis |
| `venice-uncensored` | 32K | — | Uncensored, creative, roleplay |

### Fast Models

Optimized for speed and low latency.

| Model | Context Window | Capabilities | Best For |
|-------|---------------|--------------|----------|
| `glm-4.7-flash` | 200K | Function Calling, Reasoning | Agentic coding, tool-use workflows, local deployment |
| `llama-3.2-3b` | 128K | Function Calling | Fastest responses, simple tasks |
| `qwen3-4b` | 32K | Function Calling, Reasoning | Lightweight, mobile, low-latency |

## Embeddings Models

For vector embeddings and semantic search.

| Model | Best For |
|-------|----------|
| `text-embedding-bge-m3` | Text embeddings, RAG, semantic search |

## Audio Models

### Text-to-Speech

| Model | Best For |
|-------|----------|
| `tts-kokoro` | Natural-sounding voice synthesis |

{/* ### Speech-to-Text

| Model | Best For |
|-------|----------|
| `whisper-v3-large-turbo` | Transcription, audio processing | */}

## Model Capabilities

<AccordionGroup>
<Accordion title="Function Calling">
Models with function calling can invoke tools and APIs. Use the `tools` parameter in your chat completion request to define available functions.

**Supported models:** Most models except `venice-uncensored` and `hermes-3-llama-3.1-405b`
</Accordion>

<Accordion title="Reasoning">
Reasoning models support extended thinking and step-by-step problem solving. They're optimized for complex math, logic, and analytical tasks.

**Supported models:** `glm-5`, `kimi-k2.5`, `kimi-k2-thinking`, `glm-4.7-thinking`, `glm-4.7-flash`, `glm-4.7`, `qwen3-4b`, `minimax-m2.5`
</Accordion>

<Accordion title="Vision">
Vision-capable models can analyze images passed in the messages array.

**Supported models:** `mistral-31-24b`, `kimi-k2.5`, `minimax-m2.5`
</Accordion>

<Accordion title="Web Search">
Any model can be upgraded with real-time web search by appending `:web` to the model name. See the [Web Search](#web-search-with-web) section below for details.

**Available for:** All models
</Accordion>

<Accordion title="Code Optimization">
Models specifically optimized for code generation and programming tasks.

**Supported models:** `qwen3-coder-480b-a35b-instruct`, `kimi-k2-thinking`, `glm-5`, `minimax-m2.5`
</Accordion>
</AccordionGroup>

## Web Search with `:web`

Every model listed above can be enhanced with real-time web search capabilities — you don't need a separate model for it. Simply append `:web` to any model name, and the model will search the internet for current information before generating its response.

<Note>
The model tables above only list base model names. To use any model **with web search**, just add `:web` to the end. For example, `llama-3.3-70b` becomes `llama-3.3-70b:web`. This works universally across every model in the Morpheus marketplace.
</Note>

| Base Model | With Web Search | What Changes |
|------------|----------------|--------------|
| `glm-5` | `glm-5:web` | Adds real-time internet search to responses |
| `kimi-k2.5` | `kimi-k2.5:web` | Combines deep reasoning with current web data |
| `glm-4.7-flash` | `glm-4.7-flash:web` | Adds web search to fast, efficient responses |
| _any model_ | _model-name_`:web` | Same pattern — works for all models |

<Tip>
**Verify exact model names** — including `:web` variants — by querying the [`/models` endpoint](/api-reference/models/all-models). The `:web` suffix is universal, but the base model name must match exactly what the API returns.
</Tip>

## Using Models

Specify the model ID in your API requests:

<Tabs>
<Tab title="curl">
```bash
curl https://api.mor.org/api/v1/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-3.3-70b",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ]
  }'
```
</Tab>

<Tab title="Python">
```python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_API_KEY",
    base_url="https://api.mor.org/api/v1"
)

response = client.chat.completions.create(
    model="llama-3.3-70b",
    messages=[
        {"role": "user", "content": "Hello!"}
    ]
)
```
</Tab>

<Tab title="JavaScript">
```javascript
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "YOUR_API_KEY",
  baseURL: "https://api.mor.org/api/v1",
});

const response = await client.chat.completions.create({
  model: "llama-3.3-70b",
  messages: [
    { role: "user", content: "Hello!" }
  ],
});
```
</Tab>
</Tabs>

## List Active Models

Query the API to see currently available models:

```bash
curl https://api.mor.org/api/v1/models \
  -H "Authorization: Bearer YOUR_API_KEY"
```

<Tip>
Model availability depends on active providers in the Morpheus Inference Marketplace. The API automatically routes your request to the highest-rated provider for your selected model.
</Tip>

## Model Selection Guide

<AccordionGroup>
<Accordion title="Best for coding">
- **`qwen3-coder-480b-a35b-instruct`** - Top choice for code generation (256K context)
- **`minimax-m2.5`** - SOTA agentic coding, full-stack development (1M context)
- **`glm-5`** - Agentic engineering, multi-file systems design (200K context)
- **`kimi-k2-thinking`** - Best for complex algorithmic problems with reasoning
- **`llama-3.3-70b`** - Good balance of speed and quality
</Accordion>

<Accordion title="Best for long documents">
- **`minimax-m2.5`** - 1M context window
- **`qwen3-next-80b`** - 256K context window
- **`kimi-k2.5`** - 256K context with multimodal reasoning
- **`qwen3-coder-480b-a35b-instruct`** - 256K context window
- **`glm-5`** - 200K context, excellent at document analysis
- **`kimi-k2-thinking`** - 256K context with reasoning
</Accordion>

<Accordion title="Best for speed">
- **`glm-4.7-flash`** - 30B MoE (3B active), 200K context, runs on consumer GPUs
- **`qwen3-4b`** - Fastest, 32K context
- **`llama-3.2-3b`** - Very fast, 128K context
- **`mistral-31-24b`** - Good speed with vision support
</Accordion>

<Accordion title="Best for reasoning">
- **`kimi-k2.5`** - Top math/logic benchmarks (AIME 96%), multimodal, 256K context
- **`glm-5`** - Agentic engineering, systems reasoning, 200K context
- **`kimi-k2-thinking`** - Deep reasoning chains, 256K context
- **`glm-4.7-thinking`** - Extended thinking mode, 198K context
- **`qwen3-235b`** - Complex analysis, 128K context
</Accordion>

<Accordion title="Best for AI agents">
- **`minimax-m2.5`** - Purpose-built for agents, 80.2% SWE-Bench, multi-step tool orchestration
- **`glm-5`** - Long-horizon agentic tasks, #1 open-source on Vending Bench 2
- **`kimi-k2.5`** - Agent Swarm with up to 100 parallel sub-agents
- **`glm-4.7-flash`** - Lightweight agentic coding, efficient tool-use workflows
</Accordion>

<Accordion title="Best for uncensored/creative">
- **`venice-uncensored`** - Minimal content restrictions, roleplay
</Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={2}>
<Card title="Quickstart" icon="rocket" href="/quickstart">
  Get started making your first API call.
</Card>

<Card title="Chat Completions" icon="message" href="/api-reference/chat/completions">
  Full API reference for chat completions.
</Card>

<Card title="Embeddings" icon="magnifying-glass" href="/api-reference/embeddings/create-embeddings">
  Create embeddings for semantic search.
</Card>

<Card title="Text-to-Speech" icon="volume-high" href="/api-reference/audio/speech">
  Generate speech from text.
</Card>
</CardGroup>
